{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valkryhx/Firefly/blob/master/gptq_lora_baichuan7b_vicuna_v2_%E6%94%AF%E6%8C%81%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83_%E5%92%8C%E7%9B%AE%E5%BD%95%E4%BD%9C%E4%B8%BA%E6%95%B0%E6%8D%AE%E9%9B%86_%E8%A7%A3%E5%86%B3%E4%BA%86type%E4%B8%BAbool%E7%9A%84argument%E7%9A%84bug_20230723.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuRutEY-GzfP",
        "outputId": "90d50606-5b9f-4073-fc0e-63c2b0428f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Firefly'...\n",
            "remote: Enumerating objects: 750, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 750 (delta 64), reused 28 (delta 28), pack-reused 606\u001b[K\n",
            "Receiving objects: 100% (750/750), 6.41 MiB | 8.11 MiB/s, done.\n",
            "Resolving deltas: 100% (432/432), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/valkryhx/Firefly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Firefly/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EFv9f1OHXR4",
        "outputId": "339bf014-a50b-42bb-de01-891942acb2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Firefly/Firefly\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 1))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-5472a192\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-5472a192\n",
            "  Resolved https://github.com/huggingface/peft.git to commit e06d94ddeb6c70913593740618df76908b918d66\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.30.2)\n",
            "Requirement already satisfied: datasets==2.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.12.0)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: loguru==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: fire==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.5.0)\n",
            "Requirement already satisfied: bitsandbytes==0.40.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.40.2)\n",
            "Requirement already satisfied: wandb==0.15.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.15.3)\n",
            "Requirement already satisfied: cpm_kernels==1.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.0.11)\n",
            "Requirement already satisfied: accelerate==0.20.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.20.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.1.99)\n",
            "Requirement already satisfied: deepspeed==0.9.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.9.5)\n",
            "Requirement already satisfied: numpy==1.25.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.5.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.12.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers_stream_generator in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.0.4)\n",
            "Requirement already satisfied: auto-gptq==0.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (0.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.3.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.6)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.32)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.28.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.11)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 18)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 18)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 18)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 18)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 18)) (2.0.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.3.0->-r requirements.txt (line 20)) (1.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 18)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 18)) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 15)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 15)) (2022.7.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 16)) (0.40.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 16)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 16)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 16)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 16)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EKSWuLH3I7JT",
        "outputId": "d1da5215-29ca-4727-bb28-aa5125724b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Firefly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd Firefly/"
      ],
      "metadata": {
        "id": "gLuNg2yuJQwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gptq + lora 训练： 从base model开始  \n",
        "# 注意 warmup_steps 优先级高 会覆盖warmup_ratio的效果\n",
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformersTrainingArguments.set_lr_scheduler.warmup_steps"
      ],
      "metadata": {
        "id": "Rwj1tOwco3LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是train_args/gptq_lora/baichuan7b_gptq_lora.json设置\"lr_scheduler_type\": \"constant_with_warmup\"的效果  "
      ],
      "metadata": {
        "id": "Mt_OXW_t2Obc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --all --force\n",
        "! python train_gptq_lora.py  --train_args_file train_args/gptq_lora/baichuan7b_gptq_lora.json \\\n",
        "                 --output_dir  output/baichuan-gptq-v1 \\\n",
        "                 --use_safetensors False \\\n",
        "                 --model_name_or_path  fireballoon/baichuan-vicuna-chinese-7b-gptq"
      ],
      "metadata": {
        "id": "eXLhm_GVo1rk",
        "outputId": "f5b291b1-04e8-4e91-f78f-abf40fd615b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching origin\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), 1.79 KiB | 1.79 MiB/s, done.\n",
            "From https://github.com/valkryhx/Firefly\n",
            "   b3123aa..db8cc39  master     -> origin/master\n",
            "Updating b3123aa..db8cc39\n",
            "Fast-forward\n",
            " train_args/gptq_lora/baichuan7b_gptq_lora.json | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
            "[2023-07-23 12:36:15,838] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-07-23 12:36:21.060042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m2023-07-23 12:36:25.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1margs.ddp_find_unused_parameters=True,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1margs.use_safetensors=False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mmark 0   ddp_find_unused_parameters = True  ,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mmark 1   lora_args.model_name_or_path = fireballoon/baichuan-vicuna-chinese-7b-gptq\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mmark 1   training_args.use_safetensors = False  ,type(training_args.use_safetensors)\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mmark 1   training_args.ddp_find_unused_parameters = None,type=<class 'NoneType'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mmark 2   training_args.ddp_find_unused_parameters = True,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mmark 3   training_args.ddp_find_unused_parameters = True,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1m【lora_args】=QLoRAArguments(max_seq_length=512, train_file='./gptq_data', model_name_or_path='fireballoon/baichuan-vicuna-chinese-7b-gptq', task_type='', eval_file='', lora_rank=8, lora_alpha=32, lora_dropout=0.05)\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1m【training_args】=TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=True,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/gptq_lora_baichuan7b-v1/runs/Jul23_12-36-25_f9ed4f489217,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=constant_with_warmup,\n",
            "max_grad_norm=0.3,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10,\n",
            "optim=paged_adamw_8bit,\n",
            "optim_args=None,\n",
            "output_dir=output/baichuan-gptq-v1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/gptq_lora_baichuan7b-v1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=5,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=15,\n",
            "weight_decay=0.0001,\n",
            "xpu_backend=None,\n",
            ")\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mmyargs=QLoRAArguments(max_seq_length=512, train_file='./gptq_data', model_name_or_path='fireballoon/baichuan-vicuna-chinese-7b-gptq', task_type='', eval_file='', lora_rank=8, lora_alpha=32, lora_dropout=0.05)\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1moriginal training_args.ddp_find_unused_parameters  = True,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mInitializing components...\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mGPTQ 模型加载开始: fireballoon/baichuan-vicuna-chinese-7b-gptq\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m254\u001b[0m - \u001b[1muse_safetensors: False  ,<class 'bool'>\u001b[0m\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "\u001b[32m2023-07-23 12:36:51.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mmemory footprint of model: 5.099815368652344 GB\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:51.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m305\u001b[0m - \u001b[1mGPTQ 模型加载完成\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:51.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mCheckpoint output/lora_baichuan/adapter_model.bin not found\u001b[0m\n",
            "trainable params: 2,097,152 || all params: 1,337,511,936 || trainable%: 0.15679501195868206\n",
            "\u001b[32m2023-07-23 12:36:51.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mGPTQ LORA模型加载完成\u001b[0m\n",
            "verify all params of the model\n",
            "torch.float32 528011264 0.39477125383948725\n",
            "torch.int32 809500672 0.6052287461605128\n",
            "torch.float32 ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight']\n",
            "\n",
            "verify trainable params the model\n",
            "torch.float32 2097152 1.0\n",
            "torch.float32 2097152\n",
            "\u001b[32m2023-07-23 12:36:51.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mdata files: ./gptq_data/1.json, ./gptq_data/2.json\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:51.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mthere are 20 data in dataset\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:51.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=True,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/gptq_lora_baichuan7b-v1/runs/Jul23_12-36-25_f9ed4f489217,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=constant_with_warmup,\n",
            "max_grad_norm=0.3,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10,\n",
            "optim=paged_adamw_8bit,\n",
            "optim_args=None,\n",
            "output_dir=output/baichuan-gptq-v1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/gptq_lora_baichuan7b-v1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=5,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=15,\n",
            "weight_decay=0.0001,\n",
            "xpu_backend=None,\n",
            ")\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:51.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mafter change training_args.ddp_find_unused_parameters  = False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 12:36:51.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1m*** starting training ***\u001b[0m\n",
            "{'loss': 1.1658, 'learning_rate': 6.666666666666667e-05, 'epoch': 1.0}\n",
            "{'loss': 1.1105, 'learning_rate': 0.00013333333333333334, 'epoch': 2.0}\n",
            "{'loss': 0.9989, 'learning_rate': 0.0002, 'epoch': 3.0}\n",
            "{'loss': 0.8184, 'learning_rate': 0.0002, 'epoch': 4.0}\n",
            "{'loss': 0.5926, 'learning_rate': 0.0002, 'epoch': 5.0}\n",
            "{'loss': 0.3719, 'learning_rate': 0.0002, 'epoch': 6.0}\n",
            "{'loss': 0.185, 'learning_rate': 0.0002, 'epoch': 7.0}\n",
            "{'loss': 0.0759, 'learning_rate': 0.0002, 'epoch': 8.0}\n",
            "{'loss': 0.0361, 'learning_rate': 0.0002, 'epoch': 9.0}\n",
            "{'loss': 0.0281, 'learning_rate': 0.0002, 'epoch': 10.0}\n",
            "{'train_runtime': 2474.2504, 'train_samples_per_second': 0.081, 'train_steps_per_second': 0.02, 'train_loss': 0.5383205151557923, 'epoch': 10.0}\n",
            "100% 50/50 [41:14<00:00, 49.48s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.5383\n",
            "  train_runtime            = 0:41:14.25\n",
            "  train_samples_per_second =      0.081\n",
            "  train_steps_per_second   =       0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gptq + lora 继续训练：从base model挂载上次的lora adapter 继续lora训练"
      ],
      "metadata": {
        "id": "jO-6wr-GpMi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 注意代码中--ddp_find_unused_parameters的传参方式\n",
        "#  在deepspeed和torchrun 做分布式训练时 必须让这个参数为False 要显式的指明为False\n",
        "否则报错 这个报错能查到就是这个参数的锅\n",
        "RuntimeError: Expected to mark a variable ready only once. This error is caused\n",
        "by one of the following reasons: 1) Use of a module parameter outside the\n",
        "`forward` function. Please make sure model parameters are not shared across\n",
        "multiple concurrent forward-backward passes. or try to use _set_static_graph()\n",
        "as a workaround if this module graph does not change during training loop.2)\n",
        "Reused parameters in multiple reentrant backward passes. For example, if you use\n",
        "multiple `checkpoint` functions to wrap the same part of your model, it would\n",
        "result in the same set of parameters been used by different reentrant backward\n",
        "passes multiple times, and hence marking a variable ready multiple times. DDP\n",
        "does not support such use cases in default. You can try to use\n"
      ],
      "metadata": {
        "id": "0nWdUgR35LDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --all --force\n",
        "# !torchrun --nproc-per-node=1\n",
        "!deepspeed --num_gpus=1 train_gptq_lora.py  --train_args_file train_args/gptq_lora/baichuan7b_gptq_lora.json \\\n",
        "              --peft_path  output/baichuan-gptq-v1/final   \\\n",
        "              --output_dir  output/baichuan-gptq-v2 \\\n",
        "              --ddp_find_unused_parameters False \\\n",
        "              --use_safetensors False \\\n",
        "              --model_name_or_path  fireballoon/baichuan-vicuna-chinese-7b-gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq0_aIP7H6LI",
        "outputId": "03c0cf31-e4a3-4206-b5f3-3315941b0d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching origin\n",
            "Already up to date.\n",
            "[2023-07-23 13:42:13,550] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-07-23 13:42:17.303585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-07-23 13:42:18,641] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2023-07-23 13:42:18,655] [INFO] [runner.py:555:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train_gptq_lora.py --train_args_file train_args/gptq_lora/baichuan7b_gptq_lora.json --peft_path output/baichuan-gptq-v1/final --output_dir output/baichuan-gptq-v2 --ddp_find_unused_parameters False --use_safetensors False --model_name_or_path fireballoon/baichuan-vicuna-chinese-7b-gptq\n",
            "[2023-07-23 13:42:20,185] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-07-23 13:42:24.011244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-07-23 13:42:25,789] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2023-07-23 13:42:28,693] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-07-23 13:42:31.015022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m2023-07-23 13:42:34.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1margs.ddp_find_unused_parameters=False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1margs.use_safetensors=False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mmark 0   ddp_find_unused_parameters = False  ,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mmark 1   lora_args.model_name_or_path = fireballoon/baichuan-vicuna-chinese-7b-gptq\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mmark 1   training_args.use_safetensors = False  ,type(training_args.use_safetensors)\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mmark 1   training_args.ddp_find_unused_parameters = None,type=<class 'NoneType'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mmark 2   training_args.ddp_find_unused_parameters = False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mmark 3   training_args.ddp_find_unused_parameters = False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1m【lora_args】=QLoRAArguments(max_seq_length=512, train_file='./gptq_data', model_name_or_path='fireballoon/baichuan-vicuna-chinese-7b-gptq', task_type='', eval_file='', lora_rank=8, lora_alpha=32, lora_dropout=0.05)\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_everything\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1m【training_args】=TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/gptq_lora_baichuan7b-v1/runs/Jul23_13-42-34_f9ed4f489217,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=0.3,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10,\n",
            "optim=paged_adamw_8bit,\n",
            "optim_args=None,\n",
            "output_dir=output/baichuan-gptq-v2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/gptq_lora_baichuan7b-v1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=5,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=15,\n",
            "weight_decay=0.0001,\n",
            "xpu_backend=None,\n",
            ")\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mmyargs=QLoRAArguments(max_seq_length=512, train_file='./gptq_data', model_name_or_path='fireballoon/baichuan-vicuna-chinese-7b-gptq', task_type='', eval_file='', lora_rank=8, lora_alpha=32, lora_dropout=0.05)\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1moriginal training_args.ddp_find_unused_parameters  = False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mInitializing components...\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mGPTQ 模型加载开始: fireballoon/baichuan-vicuna-chinese-7b-gptq\u001b[0m\n",
            "\u001b[32m2023-07-23 13:42:34.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m254\u001b[0m - \u001b[1muse_safetensors: False  ,<class 'bool'>\u001b[0m\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "\u001b[32m2023-07-23 13:43:13.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mmemory footprint of model: 5.099815368652344 GB\u001b[0m\n",
            "\u001b[32m2023-07-23 13:43:13.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m305\u001b[0m - \u001b[1mGPTQ 模型加载完成\u001b[0m\n",
            "\u001b[32m2023-07-23 13:43:13.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1mContinue training  from output/baichuan-gptq-v1/final/adapter_model.bin\u001b[0m\n",
            "trainable params: 2,097,152 || all params: 1,337,511,936 || trainable%: 0.15679501195868206\n",
            "\u001b[32m2023-07-23 13:43:13.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mGPTQ LORA模型加载完成\u001b[0m\n",
            "verify all params of the model\n",
            "torch.float32 528011264 0.39477125383948725\n",
            "torch.int32 809500672 0.6052287461605128\n",
            "torch.float32 ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight']\n",
            "\n",
            "verify trainable params the model\n",
            "torch.float32 2097152 1.0\n",
            "torch.float32 2097152\n",
            "\u001b[32m2023-07-23 13:43:13.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mdata files: ./gptq_data/1.json, ./gptq_data/2.json\u001b[0m\n",
            "\u001b[32m2023-07-23 13:43:13.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mthere are 20 data in dataset\u001b[0m\n",
            "\u001b[32m2023-07-23 13:43:13.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/gptq_lora_baichuan7b-v1/runs/Jul23_13-42-34_f9ed4f489217,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=0.3,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10,\n",
            "optim=paged_adamw_8bit,\n",
            "optim_args=None,\n",
            "output_dir=output/baichuan-gptq-v2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/gptq_lora_baichuan7b-v1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=5,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=15,\n",
            "weight_decay=0.0001,\n",
            "xpu_backend=None,\n",
            ")\u001b[0m\n",
            "\u001b[32m2023-07-23 13:43:13.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_components\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mafter change training_args.ddp_find_unused_parameters  = False,type=<class 'bool'>\u001b[0m\n",
            "\u001b[32m2023-07-23 13:43:13.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1m*** starting training ***\u001b[0m\n",
            "{'loss': 0.0139, 'learning_rate': 6.666666666666667e-05, 'epoch': 1.0}\n",
            "{'loss': 0.0097, 'learning_rate': 0.00013333333333333334, 'epoch': 2.0}\n",
            "{'loss': 0.0105, 'learning_rate': 0.0002, 'epoch': 3.0}\n",
            "{'loss': 0.0196, 'learning_rate': 0.0001900968867902419, 'epoch': 4.0}\n",
            "{'loss': 0.0082, 'learning_rate': 0.00016234898018587337, 'epoch': 5.0}\n",
            "{'loss': 0.0118, 'learning_rate': 0.00012225209339563145, 'epoch': 6.0}\n",
            "{'loss': 0.008, 'learning_rate': 7.774790660436858e-05, 'epoch': 7.0}\n",
            "{'loss': 0.005, 'learning_rate': 3.7651019814126654e-05, 'epoch': 8.0}\n",
            "{'loss': 0.0018, 'learning_rate': 9.903113209758096e-06, 'epoch': 9.0}\n",
            "{'loss': 0.0012, 'learning_rate': 0.0, 'epoch': 10.0}\n",
            "{'train_runtime': 2538.493, 'train_samples_per_second': 0.079, 'train_steps_per_second': 0.02, 'train_loss': 0.008968213591724634, 'epoch': 10.0}\n",
            "100% 50/50 [42:18<00:00, 50.77s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =      0.009\n",
            "  train_runtime            = 0:42:18.49\n",
            "  train_samples_per_second =      0.079\n",
            "  train_steps_per_second   =       0.02\n",
            "[2023-07-23 14:25:34,411] [INFO] [launch.py:347:main] Process 68991 exits successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "dFw5V7Wsp8om",
        "outputId": "c836e0ae-9665-4047-9470-6450d6bcd455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Firefly/Firefly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 测试sft后的单轮对话效果"
      ],
      "metadata": {
        "id": "HNwmBowcQmZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --all --force\n",
        "!python test_gptq_lora_adapter.py \\\n",
        "     --use_safetensors False \\\n",
        "     --peft_path output/baichuan-gptq-v2/final \\\n",
        "     --base_model_name_or_path fireballoon/baichuan-vicuna-chinese-7b-gptq"
      ],
      "metadata": {
        "id": "PsrzsrHymZyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ee7baf-9e6c-4ae8-d7bd-d1ad9313669c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching origin\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 917 bytes | 50.00 KiB/s, done.\n",
            "From https://github.com/valkryhx/Firefly\n",
            "   76b0f1f..31a7237  master     -> origin/master\n",
            "Updating 76b0f1f..31a7237\n",
            "Fast-forward\n",
            " test_gptq_lora_adapter.py | 5 \u001b[32m++++\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 4 insertions(+), 1 deletion(-)\n",
            "[2023-07-23 14:36:52,230] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-07-23 14:36:58.576972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m2023-07-23 14:37:02.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1margs.use_safetensors= False , <class 'bool'>\u001b[0m\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "PeftModelForCausalLM(\n",
            "  (base_model): GPTQLoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): FusedLlamaAttentionForQuantizedModel(\n",
            "              (qkv_proj): GeneralQuantLinear(in_features=4096, out_features=12288, bias=True)\n",
            "              (o_proj): GPTQLoraLinear(\n",
            "                in_features=4096, out_features=4096, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (linear_module): GeneralQuantLinear(in_features=4096, out_features=4096, bias=True)\n",
            "              )\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (act_fn): SiLUActivation()\n",
            "              (down_proj): GeneralQuantLinear(in_features=11008, out_features=4096, bias=True)\n",
            "              (gate_proj): GeneralQuantLinear(in_features=4096, out_features=11008, bias=True)\n",
            "              (up_proj): GeneralQuantLinear(in_features=4096, out_features=11008, bias=True)\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm()\n",
            "            (post_attention_layernorm): LlamaRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "User：请问怎样避免在项目中出现拖延\n",
            "Bot：避免拖延的方法有很多,以下是一些可行的建议:\n",
            "\n",
            "1. 制定计划和时间表:在项目开始时,制定一个详细的计划和时间表,包括每个任务的截止日期和所需时间。这样可以确保您有足够的时间来完成每个任务,并且能够更好地管理时间。\n",
            "\n",
            "2. 分解任务:将大型任务分解成更小、更具体的任务,这样您就可以更好地管理时间和进度。通过分解任务,您可以更好地掌握每个任务的优先级和所需时间。\n",
            "\n",
            "3. 减少干扰:在工作期间,尽量避免干扰和分心。关闭手机通知,将电子邮件通知设置为仅在特定时间显示,避免与同事过多的交流等。\n",
            "\n",
            "4. 留出缓冲时间:在计划和时间表中留出一些缓冲时间,以便在出现紧急情况或出现延误时进行调整。\n",
            "\n",
            "5. 与他人交流:如果您预计无法按时完成任务,请及早与团队成员或领导进行沟通。这样可以确保团队明确任务的状态,并且可以及时调整计划和时间表。\n",
            "\n",
            "6. 奖励自己:在完成重要任务或任务阶段后,给自己一个小小的奖励,这样可以提高自我动力,鼓励自己更好地管理时间和计划。\n",
            "User：发票报销的流程\n",
            "Bot：1. 发票应在开票日期的15天内进行报销。\n",
            "2. 员工应在发票上注明报销金额和报销日期。\n",
            "3. 员工应将发票和报销单一起提交给财务部门。\n",
            "4. 财务部门应审核发票和报销单,确保金额和日期正确。\n",
            "5. 财务部门应在发票上盖章并归档。\n",
            "6. 员工应在公司报销系统中提交报销申请。\n",
            "7. 财务部门应在公司报销系统中审核申请并确认报销金额。\n",
            "8. 财务部门应在公司报销系统中生成报销发票。\n",
            "9. 财务部门应在公司报销系统中记录报销明细和报销金额。\n",
            "10. 财务部门应在公司财务报表中记录报销金额。\n",
            "User：从武汉怎么去美国\n",
            "Bot：您可以从武汉飞往美国的主要城市，例如纽约、洛杉矶和旧金山。以下是飞往这些城市的航班的一些建议：\n",
            "\n",
            "1. 纽约肯尼迪国际机场(JFK)：从武汉飞往纽约肯尼迪国际机场的航班，航程约为14小时。\n",
            "\n",
            "2. 洛杉矶国际机场(LAX)：从武汉飞往洛杉矶国际机场的航班，航程约为15小时。\n",
            "\n",
            "3. 旧金山国际机场(SFO)：从武汉飞往旧金山国际机场的航班，航程约为16小时。\n",
            "\n",
            "4. 纽瓦克自由国际机场(EWR)：从武汉飞往纽瓦克自由国际机场的航班，航程约为13小时。\n",
            "\n",
            "5. 芝加哥奥黑尔国际机场(ORD)：从武汉飞往芝加哥奥黑尔国际机场的航班，航程约为12小时。\n",
            "\n",
            "需要注意的是，航班时间可能会因季节变化和航空公司计划而有所不同。建议您提前查看航班时间，并选择最方便和最经济的航班。\n",
            "User：介绍一下烽火通信\n",
            "Bot：烽火通信是一家通信设备制造商,为电信运营商、企业和政府部门提供产品和解决方案。该公司成立于1998年,总部位于湖北武汉,在全球各地设有分支机构。\n",
            "\n",
            "烽火通信提供一系列产品和解决方案,包括无线和有线网络设备、传输设备、数据通信设备、电源设备等。该公司专注于开发创新技术和产品,以提高通信系统的效率和可靠性。\n",
            "\n",
            "在5G领域,烽火通信提供一系列5G移动终端、5G核心网络设备和5G解决方案。该公司还提供一系列5G测试设备和解决方案,包括5G无线测试仪、5G网络切片测试仪、5G毫米波测试仪等。\n",
            "\n",
            "在数据通信领域,烽火通信提供一系列数据通信设备和解决方案,包括路由器、交换机、无线局域网设备等。该公司专注于开发创新技术和产品,以提高数据通信系统的效率和可靠性。\n",
            "\n",
            "在电源领域,烽火通信提供一系列电源设备和解决方案,包括电源、电池、不间断电源等。该公司专注于开发创新技术和产品,以提高电源系统的效率和可靠性。\n",
            "\n",
            "总的来说,烽火通信是一家实力雄厚的通信设备制造商,提供一系列产品和解决方案,包括无线和有线网络设备、传输设备、数据通信设备、电源设备等。该公司专注于开发创新技术和产品,以提高通信系统的效率和可靠性。\n",
            "User：华为强大吗\n",
            "Bot：华为的强大，是建立在无数中国人的努力和奋斗之上的。华为的成功，是无数中国企业成功的缩影，是中国经济发展的一个缩影。华为的强大，是无数中国人的强大。\n",
            "User：华为和烽火 哪个企业更好\n",
            "Bot：华为和烽火都是通信行业的企业,华为是国内最大的通信设备制造商,而烽火则是国内最大的光纤光缆生产商之一。两个企业都有很强的实力,但具体哪个更好,很难说,因为这两个企业都有自己的优势和劣势。 华为在通信设备制造方面具有很强的实力,其产品广泛应用于电信网络、企业网络和家庭网络等各个领域。华为的产品包括无线网络、有线网络、云计算、数据中心等多个领域的产品。 烽火在光纤光缆生产方面具有很强的实力,其产品广泛应用于电信网络、企业网络和家庭网络等各个领域。烽火的产品包括光纤、光缆、ODN设备、光模块等多个领域的产品。 两个企业都有很强的实力,但具体哪个更好,很难说,因为这两个企业都有自己的优势和劣势。\n",
            "User：怎么用投针法计算π\n",
            "Bot：投针法计算π的步骤如下:\n",
            "\n",
            "1. 在一个直线上面画上n个点,这些点代表区间。\n",
            "\n",
            "2. 在每个点之间画一条垂直线。\n",
            "\n",
            "3. 找出这些垂直线的交点。这些交点代表π的整数部分。\n",
            "\n",
            "4. 找出这些交点之间的距离。这个距离称为“针”。\n",
            "\n",
            "5. 找出这些交点和原点之间的距离。这个距离称为“尺”。\n",
            "\n",
            "6. 将“针”的长度除以“尺”的长度。这个商就是π的近似值。\n",
            "\n",
            "7. 重复步骤5,直到得到π的精确值。\n",
            "\n",
            "8. 将得到的π值写下来或记录下来。\n",
            "\n",
            "注意: 投针法计算π需要一些数学知识和计算能力。如果您没有这些知识,您可以使用计算器或其他方法来计算π的近似值。\n",
            "User：你说的是错误的\n",
            "Bot：您可以提供更多的信息吗? 我很想了解您的问题。\n",
            "User：你会说西班牙语吗\n",
            "Bot：当然,我会说西班牙语。你呢?\n",
            "User：那你讲几句我听听\n",
            "Bot：当然可以！您想听什么？\n",
            "User：我爱你怎么说\n",
            "Bot：如果你想表达对他的爱,你可以这样说:\n",
            "\n",
            "1. 我爱你\n",
            "\n",
            "2. 谢谢你\n",
            "\n",
            "3. 我很感激\n",
            "\n",
            "4. 我很幸福\n",
            "\n",
            "5. 我很幸运\n",
            "\n",
            "6. 你是最好的\n",
            "\n",
            "7. 我喜欢和你在一起\n",
            "\n",
            "8. 我想和你一起度过以后的岁月\n",
            "\n",
            "9. 我希望永远和你在一起\n",
            "\n",
            "10. 我爱你,谢谢。\n",
            "User：^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多轮对话 感觉user_input 只输入一次会卡住 要第二次输入才能进入While True循环 还有bug 再调试调试"
      ],
      "metadata": {
        "id": "f3tYAWEKVsmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --all --force\n",
        "!python test_multi_trun_chat_gptq_baichuan7b.py \\\n",
        "     --use_safetensors False \\\n",
        "     --peft_path output/baichuan-gptq-v2/final \\\n",
        "     --base_model_name_or_path fireballoon/baichuan-vicuna-chinese-7b-gptq"
      ],
      "metadata": {
        "id": "D8BdjRGgQtmO",
        "outputId": "d4715f54-25ad-4b5d-e507-17d77c5c4d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching origin\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 727 bytes | 727.00 KiB/s, done.\n",
            "From https://github.com/valkryhx/Firefly\n",
            "   fb8b1d6..2c65c6e  master     -> origin/master\n",
            "Updating fb8b1d6..2c65c6e\n",
            "Fast-forward\n",
            " test_multi_trun_chat_gptq_baichuan7b.py | 4 \u001b[32m++++\u001b[m\n",
            " 1 file changed, 4 insertions(+)\n",
            "[2023-07-23 15:15:15,517] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "2023-07-23 15:15:20.711783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m2023-07-23 15:15:24.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1margs.use_safetensors= False , <class 'bool'>\u001b[0m\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "PeftModelForCausalLM(\n",
            "  (base_model): GPTQLoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): FusedLlamaAttentionForQuantizedModel(\n",
            "              (qkv_proj): GeneralQuantLinear(in_features=4096, out_features=12288, bias=True)\n",
            "              (o_proj): GPTQLoraLinear(\n",
            "                in_features=4096, out_features=4096, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (linear_module): GeneralQuantLinear(in_features=4096, out_features=4096, bias=True)\n",
            "              )\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (act_fn): SiLUActivation()\n",
            "              (down_proj): GeneralQuantLinear(in_features=11008, out_features=4096, bias=True)\n",
            "              (gate_proj): GeneralQuantLinear(in_features=4096, out_features=11008, bias=True)\n",
            "              (up_proj): GeneralQuantLinear(in_features=4096, out_features=11008, bias=True)\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm()\n",
            "            (post_attention_layernorm): LlamaRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "User：你会讲英语吗\n",
            "User：你会讲英语吗\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "Bot：不会,但我可以帮你翻译。 当然,这是英语的:\n",
            "\n",
            "“嘿，兄弟，我只是想告诉你，我非常感激你能来参加这个聚会。我很高兴见到你，并与你分享这个夜晚。你能告诉我一些关于你自己的事情吗？”\n",
            "User：那你帮我翻译吧\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "Bot：当然，这是翻译过来的：\n",
            "\n",
            "“嘿，兄弟，我只是想告诉你，我非常感激你能来参加这个聚会。我很高兴见到你，并与你分享这个夜晚。你能告诉我一些关于你自己的事情吗？”\n",
            "User：翻译成英文\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "Bot：当然，这是翻译过来的：\n",
            "\n",
            "“嘿，兄弟，我只是想告诉你，我非常感激你能来参加这个聚会。我很高兴见到你，并与你分享这个夜晚。你能告诉我一些关于你自己的事情吗？”\n",
            "User："
          ]
        }
      ]
    }
  ]
}